{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uJZvJdi5VprOQHROtJIHy0mnY2afjNlx","timestamp":1686419570680}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0m2JWFliFfKT"},"source":["from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CNN model definition\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        \n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(1, 32, 3, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.Conv2d(32, 32, 3, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.Conv2d(32, 32, 3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.MaxPool2d(2, 2),\n","            nn.Dropout(0.25)\n","        )\n","        \n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(32, 64, 3, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(64, 64, 3, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(64, 64, 3, stride=2, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.MaxPool2d(2, 2),\n","            nn.Dropout(0.25)\n","        )\n","        \n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(64, 128, 3, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(128),\n","            nn.MaxPool2d(2, 2),\n","            nn.Dropout(0.25)\n","        )\n","        \n","        self.fc = nn.Sequential(\n","            nn.Linear(128, 10)\n","        )\n","                \n","        \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        \n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        x = F.log_softmax(x, dim=1)\n","        return x\n","\n"],"metadata":{"id":"PQaT7G4zZgvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_Cx9q2QFgM7"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input -? OUtput? RF\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","        self.conv5 = nn.Conv2d(256, 512, 3)\n","        self.conv6 = nn.Conv2d(512, 1024, 3)\n","        self.conv7 = nn.Conv2d(1024, 10, 3)\n","        self.Batch1= nn.BatchNorm2d(32),\n","        self.drop1= nn.Dropout(0.25)\n","\n","    def forward(self, x):\n","        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n","        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n","        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n","        x = F.relu(self.conv7(x))\n","        x = x.view(-1, 10)\n","        x = F.log_softmax(x, dim=1)\n","        return F.log_softmax(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdydjYTZFyi3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686419065867,"user_tz":-330,"elapsed":4435,"user":{"displayName":"Satej Panditrao","userId":"06333314183993197182"}},"outputId":"f3c483ac-58df-4e70-e786-165c6b9b17c3"},"source":["!pip install torchsummary\n","from torchsummary import summary\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 28, 28]             320\n","            Conv2d-2           [-1, 64, 28, 28]          18,496\n","         MaxPool2d-3           [-1, 64, 14, 14]               0\n","            Conv2d-4          [-1, 128, 14, 14]          73,856\n","            Conv2d-5          [-1, 256, 14, 14]         295,168\n","         MaxPool2d-6            [-1, 256, 7, 7]               0\n","            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n","            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n","            Conv2d-9             [-1, 10, 1, 1]          92,170\n","================================================================\n","Total params: 6,379,786\n","Trainable params: 6,379,786\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.51\n","Params size (MB): 24.34\n","Estimated Total Size (MB): 25.85\n","----------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-43-750c761bb12f>:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.log_softmax(x)\n"]}]},{"cell_type":"code","metadata":{"id":"DqTWLaM5GHgH"},"source":["\n","\n","torch.manual_seed(1)\n","batch_size = 128\n","\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                    transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8fDefDhaFlwH"},"source":["from tqdm import tqdm\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    pbar = tqdm(train_loader)\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMWbLWO6FuHb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686419543032,"user_tz":-330,"elapsed":458916,"user":{"displayName":"Satej Panditrao","userId":"06333314183993197182"}},"outputId":"25ffa800-9284-4b2d-8d18-8235d7cc33c0"},"source":["\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","for epoch in range(1, 20):\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-43-750c761bb12f>:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.log_softmax(x)\n","loss=0.9631327986717224 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.9830, Accuracy: 6820/10000 (68%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.210418701171875 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 22.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.1765, Accuracy: 9486/10000 (95%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.1240580677986145 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 22.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.1111, Accuracy: 9671/10000 (97%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.16094394028186798 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0862, Accuracy: 9734/10000 (97%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.008688819594681263 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0650, Accuracy: 9791/10000 (98%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.0837952122092247 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0544, Accuracy: 9828/10000 (98%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.05848206952214241 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0491, Accuracy: 9846/10000 (98%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.011660688556730747 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0509, Accuracy: 9837/10000 (98%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.10844016075134277 batch_id=468: 100%|██████████| 469/469 [00:22<00:00, 21.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0415, Accuracy: 9856/10000 (99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.013712900690734386 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0426, Accuracy: 9851/10000 (99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.06518873572349548 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0392, Accuracy: 9862/10000 (99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.016503507271409035 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 22.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0353, Accuracy: 9893/10000 (99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.023197777569293976 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0356, Accuracy: 9883/10000 (99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.01266081165522337 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 22.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0379, Accuracy: 9869/10000 (99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.0095455851405859 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0349, Accuracy: 9886/10000 (99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.042758941650390625 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0369, Accuracy: 9884/10000 (99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.012476247735321522 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0389, Accuracy: 9883/10000 (99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.02833346277475357 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 22.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0342, Accuracy: 9893/10000 (99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["loss=0.0034137198235839605 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.71it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0334, Accuracy: 9894/10000 (99%)\n","\n"]}]}]}